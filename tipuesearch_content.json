{"pages":[{"url":"/pages/about.html","text":"Contact: newerly(at)163.com","tags":"pages","title":"About"},{"url":"/zui-jin-lin-he-shi-you-yi-yi.html","text":"以下内容为文献《When is \"nearest neighbor\" meaningful?》阅读笔记。","tags":"阅读笔记","title":"最近邻何时有意义"},{"url":"/er-jin-zhi-shu-ju-gou-cheng-de-chi-san-kong-jian-ji-qi-zhuan-huan-guan-xi.html","text":"对于网络二进制数据，我们常以不同的单位进行划分统计。最细粒度的是以比特为单位，二进制数据集在${0,1}&#94;{2&#94;n}$离散空间中分布，其中$n$为二进制数据的比特个数。当以字节为单位进行统计时，二进制数据集在${i,i\\in{0,1,2,\\dots,255}}&#94;{2&#94;(n/8)}$的离散空间中分布。其它以不同粒度进行统计，情况类似。 要对不同协议的原始二进制数据进行聚类、分类，一个前提假设就是这些数据在离散空间中的分布不是随机均匀散布的，如果是随机均匀散布，则数据集不具备可识别特征，现在的研究就是要先证实这个假设前提，然后进行识别分析。因此，对协议的二进制数据进行聚类分析就是为了验证在某一离散空间中不同协议数据是否具有团簇特征。 而二进制数据在以不同基底构成的离散空间之间存在着转换关系，以比特和字节为单位的离散空间为例，以字节为单位的离散空间是将以比特为单位的离散空间，按8比特，或认为是8个维度构成的一个子空间进行压缩映射： $$f:\\mathcal {0,1}&#94;8\\rightarrow{i,i\\in{0,1,\\dots,255}}$$ 而这一映射结果实际上是每8维离散子空间中的加权曼哈顿距离，即 $$f(x;w)=x&#94;Tw$$ 其中，$x$为8维向量，$w=[2&#94;0,2&#94;1,\\dots,2&#94;7]$。因此，以粒度$p$为单位构成的离散样本空间，均可视为是由比特空间中$p$维子空间的加权曼哈顿距离构成的新的离散空间。 也就是说，当我们以字节、半字节等为单位对二进制数据进行相似性度量的时候，（半）字节数据是原始的二进制数据的加权曼哈顿距离度量之后的结果。 如果两个8比特长度的二进制数据中只有一比特位的差别，使用字节的方式进行度量，就会造成$2&#94;i,0\\ge i\\le7$的误差，此时选择汉明距离可能更合适。 为了度量样本数据集在离散空间中是否具有团簇特征，可以采用序列信息熵加以判断。网络协议的二进制数据可以视为一个多符号的离散信源，一个二进制序列的每一位出现哪个符号是随机或服从某种分布的，而前后的符号中可能存在统计依赖关系。序列信息熵可以为多源（协议）混合的样本集的聚类问题提供简单的可聚类性指标，快速判断混合样本集是否具有团簇特性，如果有，则使用聚类算法提取其中的类别信息。这为海量数据的前期筛选和启动聚类过程提供了处理门限，减少不必要的计算开销。 至于如何计算协议数据的序列信息熵，后面再做专门研究。","tags":"数据分析","title":"二进制数据构成的离散空间及其转换关系"},{"url":"/ju-lei-mo-hu-ju-lei-ji-qi-you-xiao-xing.html","text":"1. 聚类问题 如果我们把聚类问题描述为在一堆数据中寻找一种\"自然分组\"，那么我们就必须定义\"自然分组\"的含义。从什么意义上，我们能够说同一类中的样本比来自不同类的样本更为相似？这个问题实际上涉及两个独立的子问题： 怎样度量样本之间的相似性？ 怎样衡量对样本集的一种划分的好坏？ 1) 相似性度量 一种直观的做法是将样本集映射或定义到一个度量空间中，通过距离来衡量样本之间的相似性，常用的欧几里德距离便是其一，样本集在欧几里德空间中进行度量。但这个广泛使用的度量方法存在以下需要注意的问题： 如果特征空间是各向同性的并且数据大致均匀分布在各个方向上，使用欧几里德距离是合理的。 但对高维稀疏、各向不均衡的数据，这种度量方式就会存在问题 。 规范化是实现不变性的一种方法。选用欧几里德距离得到的聚类结果将不会因特征空间的平移和旋转而改变，所以数据点作刚体运动不会影响分类结果。但一般地说，对线性变换或其他会扭曲距离关系的变换是不能保证的。要实现位移和缩放的不变性，可以通过平移和缩放坐标轴使新样本集具有零均值和单位方差；要实现旋转不变性，可以旋转坐标轴使这些轴与样本协方差矩阵的特征向量平行，即主成分变换。 但不能说规范化一定是必要的。通过平移和缩放使得均值为0,方差为1的规范化方法，其出发点是有效防止某些特征仅仅因为它的数值过大而主导(dominate)距离度量。对于服从正态波动的数据进行规范化操作是合理的，但如果数据的波动是因为存在多个子类，那么规范化就不合理了，因为这将改变样本数据类内和类间在原始空间中的分布，影响分类结果的正确性。 因此，对于聚类算法，一般不对原始样本集进行规范化处理？ 几类常用相似性度量方法： Minkowski度量 $$ d(\\mathsf{x},\\mathsf{y})=(\\sum_{k=1}&#94;{d}|x_k-y_k|&#94;q)&#94;{1/q},q\\ge 1 $$ 当q=2时，即为欧几里德距离，度量的是两点之间的直线距离，只有此时能够保证距离度量具有平移和旋转不变性；当q=1时，为曼哈顿距离，度量的是两点之间的各维路径之和；当$q=\\infty$时，为切比雪夫距离，度量的是两点之间最远维度之间的距离。 Minkowski距离的缺点有二： a. 各维度无量纲，可能存在各维度数值不具有可比性，而在Minkowski度量中被同等对待了； b. 没有考虑各维度的分布情况，即前面说的各向同性问题。 Mahalanobis度量（马氏距离） 设$X_{n_1×m},Y_{n_2×m}$为两个样本集，其中$m$表示样本的维数，$n_i,i=1,2$表示样本的数量，马氏距离定义为 $$ D(Y,X)=diag(\\sqrt{(Y-\\mu_X)S_X&#94;{-1}(Y-\\mu_X)&#94;T}) $$ 其中，$\\mu_X$为$X$各维度的均值，$S_X$表示$X$的协方差矩阵。马氏距离表示，以样本集$X$的分布的中心点和方向为参考，$Y$中各样本点到$X$样本分布中心的距离。需要说明的是，利用MATLAB的mahal函数计算的马氏聚类，没有s上述公式中的开方运算，而直接是平方和，还有的计算式将$X,Y$颠倒使用，但只要计算目的明确，结果是一致的。 注意，这里要求$X$的样本数$n_1$要大于维数$m$ 。因为$rank(X_{n_1×m})\\le \\min{n_1,m}$，而$X$的协方差矩阵$S=E[(X-\\mu_X)&#94;T(X-\\mu_X)]$为$m×m$的方阵，$rank(S)\\le \\min{n_1,m}$，若$n_1 紧致 地描述其在特征空间中的分布。当然，即使$n_1>m$也不意味着$S$就有逆矩阵，因为$rank(X)$也可能小于$m$，即存在线性相关的特征向量。 下图给出了马氏距离和欧氏距离在同一样本集上的结果差异 # matlab code X = mvnrnd([0;0],[1 .9;.9 1],100); Y = [1 1;1 -1;-1 1;-1 -1]; d1 = mahal(Y,X) % Mahalanobis d1 = 1.3592 21.1013 23.8086 1.4727 d2 = sum((Y-repmat(mean(X),4,1)).&#94;2, 2) % Squared Euclidean d2 = 1.9310 1.8821 2.1228 2.0739 从上面的数值可以看出，马氏距离的结果更符合我们的直观感受，即$Y$中的两个样本点与$X$样本集的距离比另外两个与$X$样本集的距离更近，而欧氏距离没有反映出这种差别。 从上图，我们也可以直观地感受到，这种差别是将$Y$中的样本分布按照$X$的分布方向进行了伸缩变换产生的。这样就很容易让我们想起LDA(线性判别分析)算法，该算法能够寻找到一个样本集分布的有效分类方向。而从公式上看，马氏距离中的样本协方差矩阵$S_X$就是LDA算法中类内散布矩阵(scatter matrix)或离散度矩阵的$n-1$倍。也就是说，马氏距离是将$Y$的样本按照$X$的散布方向进行了投影变换，再计算与$X$中心的距离，这样可以消除变量之间相关性的影响。 这里还可以注意到，如果样本集$X$是各向正交且同分布的，也就是说$X$中样本的各维特征是统计独立的，那么协方差矩阵为单位阵$S=I$，马氏距离即为欧氏距离，也就是说，此时，不需要对样本集$Y$进行坐标变换。因此，欧氏距离是正交分布空间中的马氏距离。 夹角度量 除了尺度，可以使用方向进行相似性度量，余弦夹角是一种常用的度量函数。 $$ s(\\mathsf x,\\mathsf y)=\\frac{\\mathsf x&#94;T\\mathsf y}{\\parallel \\mathsf x\\parallel\\parallel \\mathsf y\\parallel} $$ 当两个样本向量的夹角或方向是个有意义的衡量相似性的指标时，这个度量就比较合适，其对旋转和膨胀具有不变性，但对一般的线性变换不能保证。 Jaccard相似性 Jaccard相似性可以度量两个样本(集)之间共有特征(元素)的比例，它除了可以用于数值型数据，还可度量符号型数据。 $$ J(A,B)=\\frac{|A\\cap B|}{|A\\cup B|} $$ 因此，Jaccard距离可定义为 $$ J_d(A,B)=1-J(A,B) $$ 当特征是二值的时候（取0或1）,两个样本$\\mathsf x,\\mathsf y$的Jaccard系数$J(\\mathsf x,\\mathsf y)$即为余弦系数的常数C倍。而此时，Jaccard系数也可写成 $$ s(\\mathsf x,\\mathsf y)=\\frac{\\mathsf x&#94;T\\mathsf y}{\\mathsf x&#94;T\\mathsf x+\\mathsf y&#94;T\\mathsf y-\\mathsf x&#94;T\\mathsf y} $$ 该系数有时称为Tanimoto系数（距离），常在信息检索和生物分类学中出现。 另外一种变形是 $$ s(\\mathsf x,\\mathsf y)=\\frac{\\mathsf x\\mathsf y}{dim} $$ 其中，$dim$表示特征维数，上式表示共有特征与特征维数的比例。 汉明距离 在二值情况下，汉明距离是曼哈顿距离的常数倍，在matlab中常数为向量的维数。 相关系数 互信息熵 互信息熵度量的是不同样本集在概率分布上的相似性。 度量理论中的基本问题涉及距离或相似性函数，两个向量间的相似性计算总要涉及到它们的分量值的组合。然而在许多模式识别的应用中，特征向量的各个分量常常不具有可比性。应该如何处理一个各分量代表不同物理意义的向量？一般不存在通用的方法来解答这些问题。一旦设计者选择了一个相似性函数或对数据用某种方法进行了规范化处理，就表示有额外的信息被引入来赋予这些操作物理意义。因此，在选择相似性度量函数和特征量时，要注意其中可能存在的陷阱。 2) 聚类准则 误差平方和准则 $$ J_e=\\sum_{i=1}&#94;{c}\\sum_{\\mathsf x\\in \\mathcal D_i}\\parallel \\mathsf x-\\mathsf m_i\\parallel&#94;2 $$ 其中$\\mathsf m_i=\\frac{1}{n_i}\\sum_{\\mathsf x\\in \\mathcal D_i}\\mathsf x$，$\\mathcal D_i$为第$i$个聚类，$n_i$为第i个聚类中的样本数量。这是个简单直观且应用广泛的聚类准则，也称为最小方差划分(minimum variance partition)。该准则适用于类别边界清晰，团簇特征显著，即类内数据稠密，类间样本稀疏的情况。但即使满足这一条件，该准则还是存在一个潜在的问题，即当不同聚类所包含的样本数相差较大时，将一个大的类别分割开反而有可能具有更小的误差平方和。 这个由聚类样本数不均衡导致的聚类有效性下降的问题，目前还没有得到很好解决。 将上述公式做一个简单变换可以得到下面的表达式 $$ J_e=\\frac{1}{2}\\sum_{i=1}&#94;{c}n_i\\bar{s_i} $$ 其中 $$ \\bar{s_i}=\\frac{1}{n_i&#94;2}\\sum_{\\mathsf x\\in\\mathcal D_i}\\sum_{\\mathsf x&#94;1\\in\\mathcal D_i}\\parallel\\mathsf x-\\mathsf x&#94;1\\parallel&#94;2 $$ $\\bar{s_i}$表示第$i$类里点与点之间距离平方的均值，也表明最小误差平方和准则$J_e$是用欧几里德距离作为相似性度量。这也意味着我们可以替换其中的相似性度量，来构造其它准则函数。由于基于欧几里德距离的聚类只能在样本空间中划出线性类别边界，而若替换这一相似性度量函数，引入非线性的相似性度量，则可以解决样本集在聚类空间中的非线性分布带来的问题。基于这一泛化定义，最优划分就是使得准则函数取极值的划分，并希望聚类结果能够反映出数据固有的内部结构。 散布准则 定义类内和类间散布矩阵 $$ S_W=\\sum_{i=1}&#94;cS_i $$ $$ S_B=\\sum_{i=1}&#94;c n_i(\\mathsf m_i-\\mathsf m)(\\mathsf m_i-\\mathsf m)&#94;T $$ 其中$S_i$为第$i$类的散布矩阵，$m_i$为第$i$类的均值向量，$m$为总体均值向量，$n_i$为第$i$类的样本数量。 $$ S_i=\\sum_{\\mathsf x\\in\\mathcal D_i}(\\mathsf x-\\mathsf m_i)(\\mathsf x-\\mathsf m_i)&#94;T $$ $$ \\mathsf m_i=\\frac{1}{n_i}\\sum_{\\mathsf x\\in\\mathcal D_i}\\mathsf x $$ $$ \\mathsf m=\\frac{1}{n}\\sum_{\\mathcal D}\\mathsf x=\\frac{1}{n}\\sum_{i=1}&#94;{c}n_i\\mathsf m_i $$ 类内散布矩阵和类间散布矩阵是有划分决定的，大致上，这两个量之间存在一种互补关系：如果类内离散度增大，则类间离散度就会减少。因此，当试图最小化类内离散度时，最大化类间离散度是同时进行的。 基于迹的准则 粗略地说，迹代表的是散布半径的平方，因为它正比于数据在各坐标轴方向上的方差的和。因此类内散布矩阵$S_W$的迹最小化与最小化误差平方和等价。 $$ tr[S_W]=\\sum_{i=1}&#94;{c}tr[S_i]=\\sum_{i=1}&#94;{c}\\sum_{\\mathsf x\\in\\mathcal D_i}\\parallel\\mathsf x-\\mathsf m_i\\parallel&#94;2=J_e $$ 又因为总体散布矩阵$tr[S_T]=tr[S_W]+tr[S_B]$并且$tr[S_T]$与具体的划分方式无关，所有在最小化类内准则$J_e=tr[S_W]$的同时，也最大化了类间准则$tr[S_B]=\\sum_{i=1}&#94;{c}n_i\\parallel \\mathsf m_i -\\mathsf m\\parallel&#94;2$。因此，这三个准则之间是相互等价的。 基于行列式的准则 大约来说，行列式衡量的是散布体积的平方，因为它正比于数据在各个主轴方向上方差的积。当类别数$c\\le dim$时，$S_B$会是奇异阵(原因同马氏距离中协方差矩阵$S$的逆矩阵存在性问题)，而且如果$n-c<d$时，$S_B$也会是奇异的。因此，我们选择$S_W$作为准则函数参量，这里假定$S_W$是非奇异的，尽管它也可能是奇异的。 $$ J_d=|S_W|=|\\sum_{i=1}&#94;{c}S_i| $$ 最小化$J_d$得到的划分有时候是同最小化$J_e$一致的。$J_e$会因为坐标轴的缩放而改变结果，但这个问题不会影响基于$J_d$的聚类。令$T$为非奇异阵，$x'=Tx$，则$J'_d=det(T)&#94;2J_d$，$J'_e=tr(T&#94;TT)&#94;2J_e$，$J'_d$的极值和聚类划分不会因为$x'$的线性变换而改变。所以，在未知线性变换的场合下准则$J_d$是合适的。 基于不变量的准则 为了使类内距离尽可能小，类间距离尽可能大，定义最大化某个基于$S_W&#94;{-1}S_B$准则是合理的。因为其特征值$\\lambda_i,i=1,\\dots d$衡量的是类间散布和类内散布在对应特征向量方向上的比值，因此能产生较大特征值的划分是比较令人满意的。由于矩阵$S_B$的秩不超过$c-1$，因而最多有$c-1$个非零特征值。 但好的聚类划分是指那些非零特征值较大的划分 。由于特征值具有线性变换不变性，所对应的最优划分也具有不变性，基于这类不变量，可以设计出一类等效的聚类准则函数： $$ tr[S_W&#94;{-1}S_B]=\\sum_{i=1}&#94;{d}\\lambda_i $$ $$ J_f=tr[S_T&#94;{-1}S_W]=\\sum_{i=1}&#94;d\\frac{1}{1+\\lambda_i} $$ $$ \\frac{|S_W|}{|S_T|}=\\prod_{i=1}&#94;{d}\\frac{1}{1+\\lambda_i} $$ 这里可以注意到，如果总体散布矩阵$S_T=S_W+S_B$通过旋转和缩放坐标轴变成了单位阵，那么最小化$J_f$就等价于$J_e$，最小化$|S_W|/|S_T|$等价于|S_W|。 如果通过缩放坐标轴或任何其它的线性变换可以明显观察到数据可以有多种不同的划分，那么这些可能的划分都会反映在用不变量作准则进行聚类的过程中，不变量准则函数很可能出现多个峰值的情况，因而比较难优化。 参考文献 Duda, 模式分类, p432~p468 MATLAB,pdist和mahal函数文档 2. 模糊聚类 在经典的聚类问题中，样本被认为完全属于某一类别，而在无先验知识的情况下，我们很难肯定样本对类别归属的准确性，因此对每个样本引入各类别的隶属度能够更加客观地反映数据的内部结构。引入隶属度的一类经典聚类算法便是模糊$c$-均值聚类，其在误差平方和准则的基础上为各样本做了隶属度加权，表示样本归属于某一类别的可能性。 $$ J_{fuzzy}=\\sum_{i=1}&#94;n\\sum_{j=1}&#94;c u_{ij}&#94;m\\parallel\\mathsf x_i-\\mathsf m_j\\parallel&#94;2 $$ 满足约束条件 $$ \\sum_{j=1}&#94;c u_{ij}=1,1\\le i\\le n $$ $$ u_{ij}\\ge 0, 1\\le i\\le n, 1\\le j\\le c $$ $$ \\sum_{i=1}&#94;n u_{ij}>0, 1\\le j\\le c $$ 其中，$u_{ij}$是样本$\\mathsf x_i$对类别$j$的隶属度，$m$为模糊系数。当 $$u_{ij}=1,d_{ij}=\\min_{1\\le r\\le c}d_{ir};u_{ij}=0,其它$$ 时，或者当$m=0$时，有 $$J_{fuzzy}=J_e$$ 利用拉格朗日乘子法求解$J_{fuz}$的不动点，有 $$ \\mathsf m_j&#94;*=\\frac{\\sum_{i=1}&#94;n(u_{ij})&#94;m\\mathsf x_i}{\\sum_{i=1}&#94;n(u_{ij})&#94;m} $$ $$ u_{ij}&#94;*=(\\sum_{k=1}&#94;c(\\frac{d_{ij}}{d_{ik}})&#94;{\\frac{2}{m-1}})&#94;{(-1)} $$ 其中，$d_{ij}=\\parallel\\mathsf x_i-\\mathsf m_j\\parallel$。$d_{ij}$的广义距离定义为 $$ d_{ij}&#94;2=\\parallel\\mathsf x_i-\\mathsf m_j\\parallel_A&#94;2=(\\mathsf x_i-\\mathsf m_j)&#94;TA(\\mathsf x_i-\\mathsf m_j) $$ 因此 $$ 当A=I时，d_{ij}为欧氏距离 $$ $$ 当A=S_{\\mathsf m}&#94;{-1}时，d_{ij}为马氏距离，其中，S_{\\mathsf m}&#94;{-1}=\\sum_{j=1}&#94;c(\\mathsf m_j-\\bar{\\mathsf m})(\\mathsf m_j-\\bar{\\mathsf m})&#94;T, \\bar{m}=\\frac{\\sum_{j=1}&#94;c\\mathsf m_j}{c} $$ 通过迭代$\\mathsf m_j,u_{ij}$直至收敛，得到基于隶属概率的可能的最优划分。 但与$c$-均值算法一样，只用类中心来表示类，这样只适合与发现球状类型的团簇，在很多情况下，算法对噪声数据敏感。 Bezdek等人已经证明FCM算法只能保证收敛到上述不动点，不能保证收敛到目标函数的极小值点，有时会收敛到目标函数的鞍点。 根据$u_{ij}$归一化的约束条件，样本点$\\mathsf x_i$隶属于第$j$类的概率隐含受到了聚类数目的影响，当聚类数与真实情况不符合时，会产生较大的偏差。 这里给出关于$J_{fuz}$的一个几何解释，有助于理解这个目标函数的设计原理。将$u_{ij}&#94;*$代入$J_{fuz}$，可得 $$ J_{fuzzy}&#94;*=\\sum_{i=1}&#94;n(\\sum_{j=1}&#94;cd_{ij}&#94;{2/(1-m)})&#94;{(1-m)} $$ 该式表明$J_{fuz}$最小化问题实际上是选取合理的聚类中心，使得每个样本到各聚类中心的\"调和平均距离\"最小的问题。另外，将约束条件$\\sum_{j=1}&#94;cu_{ij}=1$代入拉格朗日函数中 $$ L(J_{fuz};\\lambda)=\\sum_{i=1}&#94;n\\sum_{j=1}&#94;cu_{ij}&#94;md_{ij}&#94;2+\\lambda(1-\\sum_{j=1}&#94;cu_{ij}) $$ 求导 $$ \\frac{\\partial L(J_{fuz};\\lambda)}{\\partial u_{ij}}=mu_{ij}&#94;{m-1}d_{ij}&#94;2-\\lambda=0 $$ 得 $$ u_{ij}&#94;{m-1}d_{ij}&#94;2=\\frac{\\lambda}{m} $$ 这意味着对于两个聚类中心$\\mathsf m_p$和$\\mathsf m_q$，$u_{ip}&#94;{m-1}d_{ip}&#94;2=u_{iq}&#94;{m-1}d_{iq}&#94;2$，即样本$i$到各聚类中心的加权距离是相等的，注意，当$m=1$时，该等式不成立。因此$u_{ij}$可视为一个\"等距加权\"值。进一步地，令$v=\\max_{1\\le j\\le c}u_{ij}$，可将$J_{fuz}$改写成 $$ J_{fuz}=\\sum_{i=1}&#94;n\\sum_{j=1}&#94;c u_{ij}(u_{ij}&#94;{m-1}d_{ij}&#94;2)=\\sum_{i=1}&#94;n(v&#94;{m-1}d_v&#94;2\\sum_{j=1}&#94;c u_{ij})=\\sum_{i=1}&#94;n v&#94;{m-1}d_v&#94;2 $$ 上式与硬划分的$c$-均值聚类的准则函数相比，仅多了一个隶属度加权。 上面已经说明$J_{fuz}$表示样本到聚类中心的\"调和平均距离\"，相比于误差平方和函数，多了一个$m$系数，下面的定理可以说明为什么$m$会被称为模糊系数。 $$ \\lim_{m\\to +\\infty}u_{ij}&#94;*=(\\lim_{m\\to+\\infty}\\sum_{k=1}&#94;c(\\frac{d_{ij}}{d_{kj}})&#94;{\\frac{2}{m-1}})&#94;{-1}=(\\sum_{k=1}&#94;c\\lim_{m\\to+\\infty}(\\frac{d_{ij}}{d_{kj}})&#94;{\\frac{2}{m-1}})&#94;{-1}=\\frac{1}{c} $$ $$ \\lim_{m\\to+\\infty}\\mathsf m_j&#94;*=\\frac{\\sum_{i=1}&#94;n\\mathsf x_i}{n} $$ 由上式可见，当m指数增大时，模糊聚类的类别中心趋近于整个样本集的中心，且各样本对各类的隶属的可能性趋于均等，也就是说此时各类的聚类边界是最模糊，混淆在一起的。因此，$m$的取值关系到聚类结果的准确性，而对于$m$的取值并没有得到严格的理论分析，各文献给出了一个惯用的经验区间$[1.5,2.5]$，同时Bezdek等人从算法收敛性角度得出$m$的取值与样本数量$n$有关的结论，建议$m$取值的下界为$n/(n-2)$。但根据于剑等人的研究，对于某些数据分布，$m$的最佳取值也可能不在[1.5,2.5]的区间之内。他们人为构造了一个数据集用于模糊聚类，不同数据维数条件下，$m$的实验有效值呈单调递减， 但文章没有给出这一现象的理论分析 。 因此， 模糊系数$m$的最优值与样本数量$n$和样本维数$d$的关系目前尚无理论分析 ，直觉上，$n$越小，$d$越大，聚类边界会越模糊，为了得到好的聚类结果，$m$应该相应地减小。另外， 似乎目前没有看到当$m<1$的聚类研究 ，这个区间上聚类的结果会是怎样？ 目前，对于$m$最优值的选取都是采用遍历搜索区间，计算效率较低，也不能保证收敛的是全局最优解 。 关于$m$的取值区间，于剑在《论模糊$c$-均值算法的模糊指标》一文中证明，存在比$n/(n-2)$具实用意义的下界。通过求解$J_{fuz}&#94; $的Hessian矩阵的正定性来判断最优解$u_{ij}&#94; $的稳定性和是否是$J_{fuz}$的局部极小值，如果是，则$u_{ij}&#94;*$极可能是数据聚类的最优划分。于剑证明了以下事实 $$ (U&#94;*,\\bar{\\mathsf x})是FCM的稳定解的充要条件是\\lambda_\\max(C_X)<0.5且m\\ge \\frac{1}{1-2\\lambda_\\max(C_X)} $$ 其中，$U&#94;*=[\\frac{1}{c}]_{c×n}$， $$ C_X=\\sum_{i=1}&#94;n\\frac{(\\mathsf x_i-\\bar{\\mathsf x})(\\mathsf x_i-\\bar{\\mathsf x})&#94;T}{n\\parallel\\mathsf x_i-\\bar{\\mathsf x}\\parallel&#94;2} $$ 考虑一下，$C_X$与拉普拉斯矩阵的关系，为谱的扰动分析提供判决依据 如果令$\\lambda_1,\\lambda_2,\\dots,\\lambda_s$是矩阵$C_X$的$s$个特征值，当存在稳定解时，$C_X$是半正定的，因此有$\\lambda_i\\ge 0,1\\le i\\le s$。注意到$\\sum_{i=1}&#94;s\\lambda_i=1$，则有$1/s\\le\\lambda_\\max(C_X)\\le 1$。进而根据上述$m$与$\\lambda_\\max$的关系不等式，可得如果$1<m<\\frac{2}{s-2}$或者$s<3$，则$(U&#94; ,\\bar{\\mathsf x})$肯定不是FCM的一个稳定解。当然，我们也应该避免$(U&#94; ,\\bar{\\mathsf x})$成为稳定解，因为此时$(U&#94; ,\\bar{\\mathsf x})$作为FCM输出的结果可能性很大，且这种数据划分是无意义的聚类，$(U&#94; ,\\bar{\\mathsf x})$为此也被称为FCM的平凡解。基于这一结论，可以构造两个选择模糊指标的规则。 规则1 $m\\le\\frac{1}{1-2\\lambda_\\max(C_X)}$，如果$\\lambda_\\max(C_X)<0.5$ 规则2 $m\\le\\frac{s}{s-2}$，如果$s\\ge 3$ 规则1是在破坏FCM稳定解的充要条件的基础上得到的，而规则2是规则1的近似，为了减小计算量，避免对$C_X$进行特征值分解。`如果遵守规则1或2，$(U&#94;*,\\bar{\\mathsf x})$由于是不稳定解，实际被FCM作为聚类结果输出的可能性很小。故满足规则1的$m$皆可被视为理论上有效的FCM算法模糊指标。 当数据集的维数为1或者2时，$\\lambda_\\max(C_X)\\ge0.5$恒成立，$(U&#94;*,\\bar{\\mathsf x})$恒为FCM的不稳定解，因此，对于任意的1维或2维数据，以及任意数据集只要其$\\lambda_\\max(C_X)\\ge 0.5$，理论上，任意$m$皆可为理论上有效的FCM算法模糊指标。以上的结论说明$\\lambda_\\max(C_X)$是数据自身的一个统计量，因此选择模糊指标$m$也依赖于数据本身。 参考文献 范九伦, FCM算法中隶属度的新解释及其应用 高新波, 模糊$c$-均值聚类算法中加权指数$m$的研究 于剑, 关于FCM算法中的权重指数$m$的一点注记 Bezdek, FCM: The Fuzzy c-Means Clustering Algorithm 于剑, 论模糊$c$-均值算法的模糊指标 3. 聚类有效性 由于模糊聚类引入了模糊系数，$J_{fuz}$在不同模糊系数时取得的局部极小值，哪个是对数据聚类的最佳划分是一个尚无定论的问题，该问题也被称为聚类有效性(cluster validity)问题。广义上讲，聚类有效性包括聚类质量的度量、聚类算法对某种特殊数据集的适用程度以及某种划分的最佳聚类数目。现有的模糊聚类有效性函数大致可以分为基于数据集模糊划分和基于数据集几何结构两类。 1) 基于数据集模糊划分的模糊聚类有效性函数 基于模糊划分的模糊聚类有效性函数的理论基础是好的聚类对应于数据集较\"分明\"的划分。其优点是易于计算，适用于数据量小且分布比较好的数据集，但与数据集的几何特征缺乏直接联系，对于类间有交叠的数据不能很好地处理。 Bezdek提出了第一个有效性指标——划分系数$V_{pc}(U)$，旨在度量各聚类间的重叠程度。 $$ V_{pc}(U)=\\frac{1}{n}\\sum_{i=1}&#94;n\\sum_{j=1}&#94;cu_{ij}&#94;2 $$ 同时，Bezdek还定义了一个基于信息熵的划分熵指标$V_{pe}$ $$ V_{pe}(U)=-\\frac{1}{n}\\sum_{i=1}&#94;n\\sum_{j=1}&#94;cu_{ij}\\log_{b}u_{ij} $$ 其中，$U=[u_{ij}]_{c×n}$。$(U&#94; ,c&#94; )$为最佳有效聚类划分和最佳聚类数，当满足以下条件 $$V_{pc}(U&#94; ,c&#94; )=\\max_c{\\max_{\\Omega_c}V_{pc}(U,c)}$$ $$V_{pe}(U&#94; ,c&#94; )=\\min_c{\\min_{\\Omega_c}V_{pe}(U,c)}$$ 2) 基于数据集几何结构的模糊聚类有效性函数 Xie和Beni从几何结构出发提出了基于分离性的指标$V_{xb}$，这个指标是通过极大化类间距离和极小化类内距离的形式构造的。 $$ V_{xb}=\\frac{J_{juz}}{Sep(M)}=\\frac{\\sum_{i=1}&#94;n\\sum_{j=1}&#94;cu_{ij}&#94;m\\parallel \\mathsf x_i-\\mathsf m_j\\parallel&#94;2}{n\\min_{i\\neq j}\\parallel\\mathsf m_i-\\mathsf m_j\\parallel&#94;2} $$ 3) $V_{pc},V_{pe}$的性质 定理1 当$1<c<n$时，有 1. $\\frac{1}{c}\\le V_{pc}(u,c)\\le 1$; 2. $V_{pc}(U)=1\\Leftrightarrow U是硬划分$ 3. $V_{pc}(U)=1/c\\Leftrightarrow U=[1/c]_{c×n}$ 定理2 当$1<c<n$时，有 1. $0\\le V_{pe}(u,c)\\ln c$; 2. $V_{pe}(U)=1\\Leftrightarrow U是硬划分$ 3. $V_{pe}(U)=1/c\\Leftrightarrow U=[1/c]_{c×n}$ 参考文献 范九伦, 基于可能性分布的聚类有效性 于剑, 关于聚类有效性函数FP(U,c)的研究 范九伦, 聚类有效性函数：熵公式 于剑, 关于聚类有效性函数熵公式HP(u,c)*","tags":"机器学习","title":"聚类，模糊聚类及其有效性"},{"url":"/ju-lei-yan-jiu-sui-bi-1005.html","text":"1.聚类有效性问题 聚类算法是一种无监督的学习算法，实现对各定数据的结构一无所知，无论用什么算法聚类，其聚类结果的合理性都有待评价。一些算法只能保证收敛到目标函数的局部极值，不用的聚类数和初值就可能得到不同聚类结果，如何评价不同的聚类结果，是聚类有效问题。 Bezdek在1974年提出了聚类有效性函数$V_{pc}(u,c)$，也称为划分系数(Partition Coefficient)，是第一个实用的聚类有效性标准，定义如下： $$ V_{pc}(u,c)=(1/n)*\\sum_{k=1}&#94;{n}\\sum_{i=1}&#94;{c}u_{ik}&#94;{2} $$ Bezdek指出可以以上式作为聚类有效性函数，即 $$ \\exists{u&#94; ,c&#94; }, V_{pc}(u&#94; ,c&#94; )=\\max_{2\\le c\\ge c-1}{\\max_{\\Omega_c}V_{pc}(u,c)} $$ 则以$(u&#94; ,c&#94; )$为最优聚类结果，其中$\\Omega_c$为聚类数为c时，FCM算法得到的所有划分矩阵$u$。 范九伦等人定义了一个新的聚类有效性函数$FP(u,c)=V_{pc}(u,c)-P(u,c)$，其中$P(u,c)$为新的划分系数 $$ P(u,c)=(1/c)*\\sum_{i=1}&#94;{c}(\\sum_{k=1}&#94;{n}u&#94;2_{ik}/\\sum_{k=1}&#94;{n}u_{ik}) $$ 2.对$V_{pc}(u,c)$、$P(u,c)$、$FP(u,c)$的理论分析 定理1 当$1<c<n$时， $\\frac{1}{c}\\le P(u,c)\\le 1$; $P(u,c)=1\\Leftrightarrow u是硬划分$ $P(u,c)=1/c\\Leftrightarrow u[a_1,a_2,\\dots,a_c]&#94;T× [1,1,\\dots,1],\\forall 1\\le i\\le c; a_i<0 \\sum_{i=1}&#94;{c}a_i=1$ 定理2 当$1<c<n$时， $\\frac{1}{c}\\le V_{pc}(u,c)\\le 1$; $V_{pc}(u,c)=1\\Leftrightarrow u是硬划分$ $V_{pc}(u,c)=1/c\\Leftrightarrow u=[1/c]$ 有定理1、2可知，$P(u,c)$和$V_{pc}(u,c)$具有类似的性质，于剑认为范九伦的结论未必成立，并认为： $P(u,c)$可以作为聚类有效性函数 使得$FP(u,c)$取最小值的$(u&#94; ,c&#94; )$并不能保证$V_{pc}(u&#94; ,c&#94; )$和$P(u&#94; ,c&#94; )$差别很小，一个极端的情况是根据定理1、2,当划分最模糊的时候，$FP(u,c)$为0,但非最佳划分。 这里有个问题，于剑只是定性地说明了$FP(u,c)$不可行，但没有严格的公式证明，可以计算一下这个函数的极值。 3.改进的划分系数 可能性划分系数由范九伦等人提出，划分系数是从模糊聚类问题中引出，模糊聚类问题可表示成下面的数据规划问题 $$ \\min J_m(U,V)=\\sum_{i=1}&#94;{n}\\sum_{j=1}&#94;{c}u_{ij}&#94;{m}d_{ij}&#94;2 $$ 满足约束条件$\\sum_{j=1}&#94;{c}u_{ij}=1,1\\le i\\le n;u_{ij}\\ge 0,1\\le i\\le n, 1\\le j\\le c$。其中，$n$是数据样本个数，$c$是聚类个数，$m$是权重因子，$m$比较合理的取值区间为1.5～2.5，一般取2（见于剑一文）。$u_{ij}$是样本点对每个类的隶属度矩阵。解决上述问题的算法即为模式C-均值聚类算法。为了确定聚类数目，Bezdek定义了$V_{pc}$，一个等价的定义是 $$ V_{pc}&#94;{'}(u,c)=\\frac{1}{n}\\sum_{i=1}&#94;{n}\\max_{j=1}&#94;{c}u_{ij} $$ 但这两个定义都缺乏与数据集的几何结构的直接联系（ 这个重要吗？ ）,这里引入两个定义 对于给定的模糊C-划分，$u_{ij}\\| x_i-V_j\\|$叫做样本$x_i$到第$j$个类的模糊偏差，其中$V_j$是模糊类$j$的类中心（ 一定要求取类中心？用类边界，如SVM算支撑向量也可以吧？ ） 对于给定的模糊C-划分，$\\sigma_j=\\sum_{i=1}&#94;{n}u_{ij}&#94;{2}|x_i-V_j|&#94;2$叫做第$j$个类的模糊变差。 模糊C-均值聚类算法适用于每类样本数相差不大且每类隶属度方差相差不大的团状数据。考虑到模糊C-均值聚类算法的适用条件，吴成茂等人定义了如下的有效性标准 $$ V&#94;{''}(U;c)=\\frac{\\min_{i=1}&#94;{c}(\\sum_{i=1}&#94;{n}u_{ij})}{\\max_{j=1}&#94;c(\\sum_{i=1}&#94;{n}u_{ij})}[V&#94;{'}(U;c)+1-\\frac{\\max_{j=1}&#94;{c}(\\sum_{i=1}&#94;{n}u_{ij}&#94;2\\|x_i-V_j\\|&#94;2)}{\\sum_{i=1}&#94;{n}\\|x_i-(\\sum_{i=1}&#94;{n}x_i)/n\\|&#94;2}] $$ 满足$V&#94;{''}(U&#94; ;c&#94; )=\\max_c|\\max_{\\Omega_c}V&#94;{''}(U;c)|$的$(U&#94; ,c&#94; )$为最佳的有效性聚类。 范九伦提出的$P(U;c)$由可能性分布引出，可能性分布描述因子由Hall和Kandel引入，定义为 $$ PDD=\\frac{\\sum_{i=1}&#94;{n}u_i&#94;2}{\\sum_{i=1}&#94;{n}u_i} $$ 类似于概率论中的均值概念，可能性分布描述因子反映了可能性分布的平均可能性，显然有$0<PDD<1$。同样的，将$V&#94;{''}$公式中的$V$替换为$P$，可以定义出新的有效性函数，因此 V(u,c)与P(u,c)是否存在等价关系 。 参考文献 于剑，关于聚类有效性函数 FP(u,c) 的研究 范九伦，可能性划分系数和模糊变差相结合的聚类有效性函数 吴成茂，基于数据划分最大信息的聚类有效性函数 深入阅读 Bezdek, Cluster validity with fuzzy sets 范九伦，基于可能性分布的聚类有效性","tags":"机器学习","title":"聚类研究随笔-1005"},{"url":"/pu-ju-lei-yan-yi-mo-xing-yu-dian-xing-suan-fa.html","text":"本文为《谱聚类广义模型与典型算法分析》（《模式识别与人工智能》2014年11月第27卷第11期）的文献阅读笔记。 谱聚类方法基于点对相似性度量，避免了原型方法的数据凸分布要求，但当图规模较大时，求解特征向量较为困难。此外，谱聚类通过Laplace算子构造数据的非线性低维嵌入，形成可聚类的区域，然而全局的相似性度量易忽略局部差异和多尺度性，在NJW算法、Ncut算法的分割结果中常造成一些局部区域不能分辨的现象，虽然有一些基于局部度量的谱聚类算法，但是他们并没有经过严格的数学论证。 谱聚类方法使用核函数（可进行非线性映射）计算样本点之间的Gram矩阵（即度量矩阵，将样本从数据空间映射到度量空间中），然后对Gram矩阵进行Laplacian变换（ 为什么是拉普拉斯变化 )，计算标准化Laplace矩阵 L 的特征值和特征向量（ 为什么要这么做 ）。将索引的特征值降序排列，前若干个最大特征值对应的特征向量构成数据的低维空间，数据在这些特征向量上的投影构成低维嵌入，如下图所示。 1.1 谱聚类与核k-means之间的关系 k-means的极小化问题可转化为迹最大化问题，核k-means的松弛问题的解可通过核PCA的特征值得到，而通过求解前K个特征值在进行聚类即为谱聚类过程。具体推演过程见Kernel K-means and Spectral Clustering。 1.2 谱聚类与权重核k-means Dhillon为每个数据点分配一个适当的权重，提出一种权重核k-means算法，并证明其与谱聚类算法的等价性，详见Weighted Graph Cuts without Eigenvectors。 2. 谱聚类与核PCA Bengio证明了谱聚类与核PCA的等价性，二者都是特征函数学习问题的特例。相似矩阵、谱聚类、积分算子、核PCA的特征值、特征向量之间的关系如下图所示。 3. 谱聚类与Laplacian特征映射 4. 谱聚类与判别分析 5. 分析与讨论 分析谱聚类与其他方法的联系有助于理解谱聚类的实际物理意义，为问题和模型的转化、计算复杂性的降低带来明显益处,同时为其他模型的改进提供思路和方法。具体表现如下： 1) 在保持非线性低秩结构发现能力的条件下，降低谱聚类在复杂问题求解中的复杂度，如将谱聚类转化为最大迹问题，避免矩阵求逆等运算。 2) 通过核PCA求解和表示谱聚类的特征向量可简化许多复杂计算过程。 3) 将谱聚类的低秩结构发现能力引入非线性判别分析等模型中，可改善模式识别的能力。 总之，从应用的角度出发，建立模型之间的内在联系的主要目的是为充分利用谱聚类的低秩结构发现能力，同时将复杂计算过程简化，以便有效地解决海量、复杂数据分析问题。 6. 典型算法分析 NJW算法 核函数->相似度矩阵->Laplace矩阵->特征向量和特征值 算法复杂度为O(n&#94;3)，不适合处理海量数据集，但在一些小规模非凸分布数据集上有较好效果。 Ncut 求解过程与NJW算法相似，计算复杂性为O(n&#94;2)。多次运行Ncut算法，随机出现不同的聚类结果，也就是说其聚类的稳定性不好，或对某些参数值敏感。 基于Nystrom方法的谱聚集 计算复杂度为O(r&#94;3+rn)，其中，r为样本数，n为数据量，通过少量样本的插值近似计算原始矩阵的特征向量，大幅降低计算复杂性。 多尺度谱聚类算法 融入分层和抽样技术的谱聚类算法 深入阅读 Dhillon I. S., Weighted Graph Cuts without Eigenvectors: A Multilevel Approach Von Luxburg U, Consistency of Spectral Clustering Rosasco L, On Learning with Integral Operators Welling M, Kernel K-means and Spectral Clustering Bengio Y, Spectral Clustering and Kernel PCA are Learning Eigenfunctions Zelnik-Manor L, Self-Tuning Spectral Clustering","tags":"机器学习","title":"谱聚类广义模型与典型算法"},{"url":"/ju-lei-yan-jiu-sui-bi-0929.html","text":"现在的问题研究路径是： 1.如何对混杂多种协议的网络数据在类别数未知的条件下进行聚类？ 目前正在验证 USCAWM算法 计算最佳聚类数的准确率，USCAWM算法在理论上有严格的证明，在小规模的测试数据集IRIS上验证结果基本正确。但该算法中有两个至关重要的参数，一个是相似度矩阵 R(Vi,Vj) ，一个是相似性判别阈值，其中 R(Vi,Vj) 是计算最佳聚类数的关键，当聚类数确定后，各种聚类算法都可做聚类分析，而不一定要像USCAWM算法中那样不断迭代选择最佳阈值。 那么，相似度矩阵的计算实际上涉及的是相似性度量的问题。 f: V×V->R 这个相似性度量函数 f() 将原始的样本空间映射为一个有助于聚类的度量空间，这里要注意两点： a. 原始样本空间的非线性分布会给采用线性划分的聚类方法带来困难。一般的解决方法是使非线性空间线性化，如采用核函数，或选取能够进行线性划分的维度。所以，从本质上讲，如果能够将样本空间线性化，聚类的难度将大大减小，也就是说，聚类的关键是能够找到一个将样本空间映射为线性度量空间的函数，事实上，这也是很多距离度量定义的初衷。 b. 有助于聚类的度量空间是指，类内紧凑，类间松弛。不论采用线性化映射还是降维选取线性划分的特征，最终目的都是要使得聚类空间是一个类内距离小，类间距离大的度量空间。 因此，利用USCAWM算法得到最佳聚类数的准确性很大程度上需要依赖于合适的相似性度量函数。于是问题转变为，假如我们不知道选取何种相似性度量函数是最佳的映射方式，不同度量方法对最佳聚类数的影响有多大？ 2.距离度量对基于USCAWM算法计算的最佳聚类数的扰动分析 刚才已经提到，距离度量实际上是将样本空间映射到测度空间的过程，这是任何聚类算法的必经步骤。但当我们对数据样本的分布特性知之甚少，特别是数据样本类型混杂多，分布不均匀的时候，如何选取一个合适的测度目前是没有一个基准测试或者衡量指标的。 这个问题的研究可以从标准的数据集上，尝试不同的度量方法给最佳聚类数造成的扰动进行分析。这个标准的数据集一般是公认的知名测试数据，很多聚类算法的基准测试或对比测试都是基于这些数据集进行的。 这样，一个新问题产生了。如果给定度量方法，数据样本空间的分布特性对聚类结果有多大影响？如何对样本空间的划分特性进行量化？ 3.数据样本分布特性对聚类结果的影响 现已查到这方面早已有相关的研究，即数据样本的划分系数。研究这个问题的目的是为了衡量一个样本数据集是否在某种测度意义上具有多大的聚类能力，也就是说假如在某一测度意义上，类别之间的混淆程度有多高。 对于网络数据而言，假如我们知道各协议数据的分布特性，我们可以得到某一测度意义上的聚类能力的上下界。 这个问题的实验方法，一是基于标准数据集，对其进行加噪，通过不同程度的分布特性的改变，来研究聚类性能受到的影响;二是研究不同协议数据的分布特性，尤其是载荷数据的随机性对聚类结果造成的影响。 4.聚类研究的最终目的 开展以上三个问题研究的最终目的是，要对多种混杂协议的网络数据集计算最佳聚类数，其中涉及如何选取距离度量函数（问题2），基于选取的度量空间计算得到的最佳聚类数的置信度有多高（问题3）。 得到了一个比较可靠的聚类数之后，就可以采取多种方法进行协议数据聚类，后续的协议特征提取就可有的放矢。 实际上，这里要研究的是 聚类有效性问题 ，以往的有效性是对同一类数据集评价不同聚类算法的结果好坏，而本研究的目的是要评价对某一聚类算法，数据集的质量多大程度上决定了结果的好坏。","tags":"机器学习","title":"聚类研究随笔-0929"},{"url":"/appshu-ju-bao-chu-bu-fen-xi.html","text":"应用层协议报文的特点： 以TCP作为传输层协议，HTTP作为应用层协议传输占很大比例，HTTP协议的载荷再封装自定义消息内容，同时也有基于XMPP自定义的通信协议; 一份报文，即一条TCP连接，在其载荷内容中可能包含多个消息; 可能存在跨流传输同一个报文的情况。 这些特点带来的问题是： 无法将应用层报文与消息样本一一对应，因为存在一份应用层报文包含多个自定义消息的情况; 消息中传输的数据内容较大，而报头较短;","tags":"数据分析","title":"APP数据包初步分析"},{"url":"/du-shu-bi-ji-pythongao-shou-zhi-lu.html","text":"此书中文名取得有点讨巧，原书名是Python黑客手册，但两个名字都跟书的内容没甚关系。这书主要介绍了一些比较实用的Python工程项目技巧，有很多在大型Python软件或库研发过程中使用的工具介绍。涉及文档、分发、测试、性能优化和一些编程技巧。有些东西对于现在个人的一些小规模代码可能不太用的上，有些还是值得借鉴。 1. 项目结构 图1-1给出了一个项目的标准的文件层次结构，包含文档、代码、setup.py、README.rst、requirements.txt以及代码中的测试案例和测试数据。 版本编号应该遵从正则表达式格式：N[.N]+[{a|b|c|rc}N][.postN][.devN] 2. 文档生成 采用Sphinx 3. 测试 使用unittest、mock和testscenarios模块 4. 高级技巧 用好装饰器、函数式编程和上下文管理器，可以使得代码简洁高效，可维护性好，也能使代码更专业一些。 5. 一些架构 多进程、异步和事件驱动架构","tags":"读书笔记","title":"[读书笔记]Python高手之路"},{"url":"/wiresharkchi-xian-an-zhuang.html","text":"Wireshark采用编译的方式安装会遇到许多依赖关系的问题，在离线环境中这么搞会很烦躁。一个简单的办法是在一台联网电脑上在线安装，然后把安装包都拷到离线电脑上，用 dpkg 来安装。 # 找一台联网电脑安装wireshark $ sudo apt-get install wireshark # wireshark的安装包会先下载到本地，对于ubuntu系统，一般放在`/var/cache/apt/archieves` 路径下。如果在联网电脑上安装没有问题，把这些deb的安装包拷贝到离线主机上，在同目录下执行 $ sudo dpkg -i libc-ares2_1.10.0-2_amd64.deb libsmi2ldbl_0.4.8+dfsg2-8ubuntu2_amd64.deb libwireshark3_1.10.6-1_amd64.deb libwireshark-data_1.10.6-1_all.deb libwiretap3_1.10.6-1_amd64.deb libwsutil3_1.10.6-1_amd64.deb wireshark_1.10.6-1_amd64.deb wireshark-common_1.10.6-1_amd64.deb # 然后就好了","tags":"软件","title":"Wireshark离线安装"},{"url":"/mongodbji-qun-bu-shu.html","text":"Mongodb集群搭建还是比较容易的一件事，只是操作稍显繁琐，有很多重复性劳动。由于官方没有发布集群配置工具，只能手动配置每一台服务器，为了节省时间和不必要的重复工作，这里给出一个一键操作Mongodb集群的方法和相应的脚本程序。 1. 集群环境设置 为了方便在集群多机条件下部署操作，需要为一些远程操作提供便利的系统和网络环境，例如集群机器的统一命名，ssh免登录认证，集群时统等等。假设我们要搭建一个5台机器组成的Mongodb数据库集群，我们选择Linux平台来部署集群，因为Mongodb在Linux上运行得比Windows平台上更稳定。 1.0 集群规划 当Mongodb数据库集群配置成一个分片+副本集的部署方案时，整个集群节点之间并没有主次之分，各节点完成相应角色任务即可。因此，我们给5台主机编号为 mongodb-[1~5] ，后续操作如无特别说明，均在 mongodb-1 主机上完成。 1.1 添加IP-主机名映射 向/etc/hosts文件中添加 IP-主机名映射表 10.0.0.1 mongodb-1 10.0.0.2 mongodb-2 ... 10.0.0.5 mongodb-5 这样在mongodb-1上可以直接通过主机名连接其余各台主机。 1.2 设置ssh免登录环境 执行 ssh-keygen -t rsa ，均采用默认设置，两次回车后在 ~/.ssh/ 路径下生成密钥，执行 sshcopyidbat.sh 向 mongodb-[2~5] 拷贝密钥，这个过程中需要输入各主机的登录密码。拷贝完成后，再从 mongodb-1 登录其余主机就不需要密码了。 1.3 设置主机名 执行脚本 changehostname.sh 1.4 拷贝文件 设置脚本 copyfiles.sh 中需要传输的文件，执行脚本向各主机拷贝 /etc/hosts 和mongodb的安装包，其中拷贝mongodb-linux-xxx.tar.gz文件时，会把mongdb的程序拷贝到/usr/bin/目录下，这样在命令行中就可直接调用mongodb的相关命令。 1.5 挂载磁盘 这是可选步骤，如果规划有专门的磁盘作为数据库存储空间，执行 mountdisk.sh 即完成各主机的磁盘格式化和挂载任务。当然，具体挂载的情况和挂载位置都可以通过修改脚本 mountdisk.sh 来设置。 2. Mongodb数据库集群设置 设置好集群环境，后面的事情就好说了，执行 deploymongodbcluster.py ，会有四个操作选项： 1. 部署集群 2. 启动集群 3. 停止集群 4. 删除集群 选项1 对应于尚未部署过的集群环境，脚本会按预先配置自动完成分片服务器的启动，配置服务器的启动，在配置服务器中设置好副本集，路由服务器的启动，以及数据库集群运维程序的后台运行。在这一过程中，会在指定位置新建文件夹用于存储分片、配置、路由服务器的数据、日志等文件，所有程序启动均已后台方式运行，开启日志功能。集群部署的具体配置，可以通过修改 deploymongodbcluster.py 脚本的头部常量来定制，分片的部署方案可以通过修改脚本中 get_shards() 函数来自定义。本案例的分片部署如下图所示： 执行选项1,可以完成整个集群的部署和启动，因此不用再执行选项2,按脚本提示执行程序后会自动退出。 选项2和3 对应于已经部署过的集群，完成集群的启动和停止，其中停止用 pkill -2 mongo 实现。启动和停止均不会对已有的数据库文件进行磁盘操作。 选项4 会删除已经部署过的数据库集群的所有文件和文件夹，清空所有磁盘数据， 所以执行此项一定要慎重 。 当然，要使四个选项相互之间能够关联执行，需要保证其部署方案是一致的，在脚本中定义好的配置，一般不用再去更改。 需要指出的是 ，本脚本没有使用配置文件的方式启动各数据库服务器，而是通过自动生成的命令在命令行中直接启动，各命令参数在脚本的常量字段中可设。 3. Mongodb数据库运维 在集群启动的最后，即路由服务器启动完成之后，脚本会提示是否启动 MongodbMaintainer ，目前该运维程序只完成数据库容量的定时监测，当数据库容量达到预警上限（本案例中设为总存储容量的80%），开始启动历史数据删除，直至数据库容量降至预警门限以下。 具体的运维策略可根据实际应用定制，例如按保存天数删除数据，按热度删除数据，按重要性删除数据等等。至于集群运行状态监测，可通过调用 mongostat 查看，集群的负载状态以及数据均衡可以通过处理相应的日志文件得到，这些功能都还有待增加，如果需要的话。 4. 集群性能测试 执行 benchmarkmongodb.py ，有两个测试方案，插入一百万条记录，测试写入性能，和插入、查询同时测试，其中插入和查询的数据量比例可设置。","tags":"Mongodb","title":"Mongodb集群部署"},{"url":"/mongodbwen-ti-ji-jin-mongodbji-qun-zhong-mongoswu-fa-qi-dong.html","text":"上周在出差前，为了切换集群的连接端口，也就是把mongos的port换一下，把mongodb集群用脚本停掉，实际上用的是 pkill -2 mongo 。改好端口之后，再启动，各分片服务器、配置服务器和副本集配置均正常启动，到了最后一步启动mongos路由服务器出错了，屏显信息看不出所以然，查看mongos的日志（所以一定要开日志）找到类似如下提示，大意如此： could not verify config server in sync: mongodb-1:57017, mongodb-2:57017 differ 就是说，集群有两台配置服务器不一致、不同步，导致路由服务无法启动。在集群中我设置有三个配置服务器。然后，就茫然了，赶着要出差的时候掉链子，当时不知道怎么弄，数据库集群就停工了半周。在网上查了一下，找到了解决方法，今天就搞定了。 很简单。网上有两个方法， 方法一 ，把mongodb-1上的config server的内容全部dump出来，在restore到mongodb-2的config server上去。再重启集群，实际上只需重启mongos即可，但还是报错，不一致。因为在restore到mongodb-2上时，有key冲突。 方法二 ，比较暴力，直接把mongodb-1上config的所有文件拿去替换mongodb-2上config文件夹中的所有内容。再重启集群，还是报错，只是报的是mongodb-1和mongodb-3的不一致，如法炮制，替换掉mongodb-3上config的文件内容，再重启就好了。","tags":"Mongodb","title":"[Mongodb问题集锦]Mongodb集群中mongos无法启动"},{"url":"/ji-yu-quan-ju-zhen-de-wu-jian-du-pu-ju-lei-suan-fa.html","text":"USCAWM(unsupervised spectral clustering algorithm based on weight matrix)算法出自《谱聚类的扰动分析》（《中国科学 E辑: 信息科学》2007年第37卷第4期:527~543），该论文以矩阵的扰动理论为工具对谱聚类进行了分析，通过理论证明得到了如下结论： 在适当的相似度函数下，聚类的类别数等于权矩阵的特征值中值大于1的特征值的个数; 在适当的相似度函数下，以权矩阵的前 k （聚类的类别数）个单位正交特征向量为列向量组成的行向量之间的夹角可以用来聚类; 在适当的相似度函数下，大于0的特征值约等于聚类结果中每类的样本个数。 这是一个相当好的结论。在聚类分析中，主要面临的以下三大类问题： 已知类别数的条件下，对样本数据集进行聚类，这个是研究的最多，成果也最多的问题; 未知类别数的条件下，对样本数据集进行最佳聚类，给出聚类性能最优的类别数; 聚类方法是否满足样本数据空间的约束条件，即是否能处理非凸分布，是否能划定非线性的聚类边界; 谱聚类方法可以解决第3个问题，能够对任意分布的数据集进行聚类，而USCAWM算法在谱聚类的基础上，在满足一定约束的条件下，解决了第2个问题。但该算法的问题也正在于这个约束条件不稳定、不普适，一些参数的选择也需要技巧和运气，这里就涉及到聚类算法的两个核心问题： 相似性度量如何定义 毫无疑问，在不考虑模糊聚类，即样本间是类别互斥的情况下，最优的相似性度量是能够使类内相似性最大，类间相似性最小，通常这种度量基于范式定义，但当聚类空间不为非线性空间时，这种度量不再有效。 聚类性能如何评价 聚类结果的评价指标有很多，指标之间各有侧重，尤其是当类别数未知时，根据聚类性能来判断最优类别数是尚未解决的一个问题。 从论文的实验结果来看，类别数的判断基本准确，符合理论分析结果。这里先梳理算法流程，然后给出python代码和重现的实验结果。 步骤1 构造样本点集 V 的加权图 G(V,E,W) ，两个样本点之间连边的权值表示样本点之间的相似程度; 步骤2 计算 W 的特征值（降序排列）和相应的单位正交特征向量; 步骤3 按大于1的特征值的个数确定类别数 k ; 步骤4 将 W 的前 k 个单位正交特征向量按列组成矩阵 M ; 步骤5 计算矩阵 M 行向量之间的余弦夹角，选取阈值flag，如果夹角大于阈值，则两个样本点归为一类，反之为不同类; 步骤6 计算所得的类别数，如果等于 k ，则结束计算，否则返回 步骤5 重新选择参数flag。","tags":"机器学习","title":"基于权矩阵的无监督谱聚类算法"},{"url":"/ju-lei-fang-fa-zong-jie.html","text":"点击这里看大图","tags":"机器学习","title":"聚类方法总结"},{"url":"/yong-pelicanda-jian-jing-tai-wang-zhan.html","text":"关于笔记工具 尝试过一些软件工具用于记录工作和学习笔记，先总结一下使用心得，再来讲Pelican。 1. Word，用Word文档按日期组织，方法原始朴素，效果甚差。 2. OneNote，Windows平台下尚可，但操作略显麻烦，有时候对笔记的整理也不方便。 3. Evernote，平台无关，在联网状态下可以，但离线状态就没什么大用了。 4. WordPress，比较经典的博客系统，可以离线搭建，也可在线部署，问题是，部署起来有点庞杂，还得装apache。另外，写笔记还得进到博客的后台系统里，在网页上书写，体验不好。 5. Tex，书写工具中的战斗机，对于日常的小笔记来讲太重型化了，除了要写带公式的学术笔记，一般不用。 6. redmine，这是个项目管理的web平台，也可以用作日常事物管理，搭建不复杂，但要搞清楚怎么用，尤其是一些项目管理的术语和标准化操作，也不是很容易的一件事。如果是做团队化项目管理，倒还是值得一用。 Pelican试用初感 Pelican是用Python编写的一套开源静态博客生成工具，可以添加文章、创建页面、增加插件，可以使用Markdown格式书写笔记，采用Jajin2模板引擎，可以方便地更换网站模板。之所以选择Pelican，除了部署简单，还因为其书写便捷，用Markdown格式书写普通的文本文件即可，这对笔记的保存和转移都很方便。 1. Pelican安装 Pelican依赖于Python，一般现在常用的Linux系统都预装有Python，如果没有，建议选择Anaconda的Python集成版，直接在Anaconda的官网上下载即可，Windows系统也可安装Python(x,y)。安装好Python之后，在Github上下载Pelican源码，然后安装。 # 能联网的话，直接git同步 $ git clone git://github.com/getpelican/pelican.git # 如果离线安装，先下载pelican-master.zip，解压 $ cd pelican $ python setup.py install # 这就装好了 2. 搭建博客 # 新建一个文件夹用于保存博客网站的所有内容 $ mkdir /path/to/the/blog $ cd /path/to/the/blog $ pelican -quickstart # 会生成一些目录和文件 # |--content 所有的文章都在这个文件夹里 # |--develop_server.sh 开启本地网络服务 # |--Makefile 博客生成工具 # |--output 静态文件生成目录 # |--pelicanconf.py 博客网站配置文件 # |--publishconf.py 发布配置文件 如果只是为了在本地做一个笔记书写工具，开启本地服务器即可，同时局域网内的主机也可访问 $./develop_server.sh start # 默认端口8000 $./develop_server.sh start 8010 # 指定端口8010 3. 写笔记 在content目录下新建note.md文档，文档需以下内容开头 Title: 文章标题 Date: 2015-08-17 Category: 类别 Tag: 标签1,标签2 正文内容…… 保存note.md，在/path/to/the/blog路径下执行以下命令，刷新页面即可看到新笔记 $ make html 4. 创建新页面 在content目录下创建pages目录，然后创建newpage.md并写入以下内容 Title : New Page Date : 2015 - 08 - 17 New content here ... 执行 make html ，即可看到新页面。 5. 更换主题 Pelican的Github仓库中提供一些主题模板，可以下载下来 git clone git://github.com/getpelican/pelican-themes.git 在/path/to/the/blog路径下执行以下命令，同时更改pelicanconf.py配置文件中的设置，即可安装相应主题 $ pelican -theme -i /path/to/the/theme # pelicanconf.py THEME = 'the_theme' $ make html 6. 设置静态文件 在笔记中时常遇到一些插图或者链接文件的情况，可以通过在markdown文档中链接这些静态文件实现。具体来讲，就是在content目录中新建如img，file之类的文件夹来存储图像、文件等静态文件。在markdown文档中书写 [image-to-show](img/image1.jpg) 来实现插图和文件链接。此外，还需要修改pelicanconfg.py的配置，在该文件中添加 STATIC_PATHS=[u'img', u'file'] 然后，执行 make html ，Pelican会将这些静态文件目录拷贝到output/static/目录中。 (先用到了这些基本功能，以后再慢慢添加)","tags":"软件","title":"用Pelican搭建静态网站"},{"url":"/linuxubuntuzhuang-ji-bi-bei-qing-dan.html","text":"装机系统为Ubuntu Kylin 14.04 trusty 64-bit 对于能联网的机器这个步骤可以跳过啦，局域网的机器为了后续方便，最好能建立本地软件源,不然当遇到缺各种依赖包的时候，可能会疯掉的。所以，磨刀不误砍柴功，建个软件源的本地镜像先。 # 找台能上网的机器先下载与之装机版本对应的软件源镜像，先装apt-mirror $sudo apt-get install apt-mirror # 新建文件夹 $sudo mkdir -p /downloads/ubuntu /downloads/ubuntu/mirror /downloads/ubuntu/skel /downloads/ubuntu/var # 配置镜像地址，系统默认的官方地址下载速度很慢，在官网上找到镜像列表，修改 # mirror.list选择一个国内镜像 $sudo vim /etc/apt/mirror.list # 修改本地镜像地址 #--------- mirror.list ---------- set base_path /downloads/ubuntu set mirror_path $base_path/mirror set skel_path $base_path/skel set var_path $base_path/var set cleanscript $var_path/clean.sh set nthreads 20 set _tilde 0 # 把常用的软件同步过来，这里用的是阿里云的软件源，如果同步与本机版本一致的软件， # 用deb即可，也可指定只同步64位软件，用deb-amd64 deb-amd64 http://mirrors.aliyun.com/ubuntu trusty main restricted universe multiverse deb-amd64 http://mirrors.aliyun.com/ubuntu trusty-security main restricted universe multiverse deb-amd64 http://mirrors.aliyun.com/ubuntu trusty-backports main restricted universe multiverse deb-amd64 http://mirrors.aliyun.com/ubuntu trusty-update main restricted universe multiverse #--------- END ---------- # 开始镜像，然后可以睡觉去了，然后再睡一觉，基本就把软件源都镜像好了 $sudo apt-mirror # 软件源镜像完成后，更新本地软件源 $sudo cp /etc/apt/sources.list /etc/apt/sources.list.backup # 先备份 $sudo vim /etc/apt/sources.list #--------- sources.list --------- deb file:///downloads/mirror/mirrors.aliyun.com/ubuntu/ trusty main restricted #--------- END ---------- # 更新软件源 $sudo apt-get update # 更新过程可能有\"无法下载\"的错误，尝试安装一下本机没有的软件， # 比如vim，build-essential，如果可以装上，则本地软件源设置成功，不行的话，就google吧 系统软件 vim build-essential jvm jre zsh vncviewer rar mp3，Ubuntu自带的Rhythmbox没有mp3解码器，要安装gstreamer1.0-fluendo-mp3和它的依赖库liboil0.3 编程环境 Pycharm Eclipse Cpp Eclipse Java CodeLite 文档编辑 Pelican Texlive Zotero 专业软件 Wireshark","tags":"杂项","title":"Linux(Ubuntu)装机必备清单"}]}